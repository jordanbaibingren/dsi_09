{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# DSI 9 Project 3: Classification of Reddit Posts\n",
    "Author Jordan Bai\n",
    "\n",
    "Objectives\n",
    "\n",
    "- Collect posts from two subreddits using Reddit's API.\n",
    "- Use NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collecting Posts from Two Subreddits Using Reddit's API\n",
    "\n",
    "The subreddits of choice were **NBA** and **MLB**. These are sports discussion threads on two different leagues, National Basketball Association and Major League Baseball. To acces the API, `.json` is added to the end of the url: https://www.reddit.com/r/mlb.json\n",
    "\n",
    "Reddit gives 25 posts **per request**. To get enough data, Reddit's API was hit **repeatedly** in a `for` loop. `time.sleep()` function was added at the end of your loop to allow for a break in between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "after = None\n",
    "url1 = 'https://www.reddit.com/r/mlb.json'\n",
    "\n",
    "for url in ['https://www.reddit.com/r/mlb.json', 'https://www.reddit.com/r/nba.json']:\n",
    "    for i in range(40):\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "    \n",
    "        res = requests.get(url, params=params, headers={'User-agent': 'mozl'})\n",
    "    \n",
    "        if res.status_code == 200:\n",
    "            json = res.json()\n",
    "            posts.extend(json['data']['children'])\n",
    "            after = json['data']['after']\n",
    "        else:\n",
    "            print(res1.status_code)\n",
    "            break\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning and Extraction\n",
    "\n",
    "As many of the posts were images, the focus of this poject would be based on the thread **titles**. The titles were extracted with the subreddit it belonged to. The sample size of each subreddit (NBA:972 vs MLB:983) was checked to be similar in order to avoid a mismatch problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([{'title':p['data']['title'], 'subreddit':p['data']['subreddit']} for p in posts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1951, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1951, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nba'] = np.where(df['subreddit']=='nba', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NBA:  952\n",
      "No. of MLB: 1003\n"
     ]
    }
   ],
   "source": [
    "print('No. of NBA: ',df.nba.sum())\n",
    "print('No. of MLB: {}'.format(1955-df.nba.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Extraction and Train-Test Split\n",
    "\n",
    "Three feature selection methods were employed (**Count Vectorizer** and **TF-IDF**), to explore which method gave a better score for this classification problem. A 75-25 train-test split was defind to evaluate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title']\n",
    "y = df['nba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "# Instantiate our Vectorizers.\n",
    "cvec = CountVectorizer(stop_words='english', max_features=1000)\n",
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                                      sublinear_tf=True,\n",
    "                                      max_df=0.5,\n",
    "                                      max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit-transform our Vectorizers on the training data and transform our testing data.\n",
    "cvec.fit(X_train)\n",
    "X_train_cvec = pd.DataFrame(cvec.transform(X_train).todense(),\n",
    "                       columns=cvec.get_feature_names())\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test).todense(),\n",
    "                       columns=cvec.get_feature_names())\n",
    "\n",
    "tvec.fit(X_train)\n",
    "X_train_tvec = pd.DataFrame(tvec.transform(X_train).todense(),\n",
    "                       columns=tvec.get_feature_names())\n",
    "X_test_tvec = pd.DataFrame(tvec.transform(X_test).todense(),\n",
    "                       columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Building and Testing\n",
    "\n",
    "Two classification methods were selected for evaluation (**Multinomial Naive Bayes Classifier** and **Logistic Regression**). The metric for determining the best model coupled with the feature extraction method is the prediction accuracy of the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes CountV - Train: 0.948051948051948, Test: 0.889344262295082\n",
      "Bayes TfidV - Train: 0.963773069036227, Test: 0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "# Train and score our Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(X_train_cvec, y_train)\n",
    "print('Bayes CountV - Train: {}, Test: {}'.format(nb.score(X_train_cvec, y_train),nb.score(X_test_cvec, y_test)))\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(X_train_tvec, y_train)\n",
    "print('Bayes TfidV - Train: {}, Test: {}'.format(nb.score(X_train_tvec, y_train),nb.score(X_test_tvec, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log CountV - Train: 0.974025974025974, Test: 0.9036885245901639\n",
      "Log TfidV - Train: 0.9692412850307587, Test: 0.8934426229508197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Angeline\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Angeline\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train and score our Logistic model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_cvec, y_train)\n",
    "print('Log CountV - Train: {}, Test: {}'.format(lr.score(X_train_cvec, y_train),lr.score(X_test_cvec, y_test)))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_tvec, y_train)\n",
    "print('Log TfidV - Train: {}, Test: {}'.format(lr.score(X_train_tvec, y_train),lr.score(X_test_tvec, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Best Performing Model\n",
    "\n",
    "Logistic Regression yielded the best training scores (>97% accuracy) but the testing score was lower at around 91% accuracy. This might indicate overfitting of the training model resulting in high biasness. On the other hand, the difference between the training and testing accuracies for Multinomial Naive Bayes Classifier were smaller (around 4%). With this classifier, feature extraction by TF-IDF had a higher testing accuracy of 92%. This was identified as the best combination in this exercise. We study the confusion matrix of the best model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 228\n",
      "False Positives: 22\n",
      "False Negatives: 26\n",
      "True Positives: 212\n"
     ]
    }
   ],
   "source": [
    "# Import the confusion matrix function.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(X_train_tvec, y_train)\n",
    "predictions = nb.predict(X_test_tvec)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
